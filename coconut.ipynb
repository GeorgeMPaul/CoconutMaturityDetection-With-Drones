{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automating Coconut Maturity Detection with Drone Technology"
      ],
      "metadata": {
        "id": "B817ZB2ubWF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "23VLcfJ8bfX7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LftxrJE4LMo3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob as glob\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import requests\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation"
      ],
      "metadata": {
        "id": "TiJiGUV0b0y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Coconut-dataset from Roboflow for training the custom YOLOv5 object detector.\n",
        "The code to download the dataset you can get from https://universe.roboflow.com/nit-calicut/coconut-veirf/dataset/5#. Do not share this snippet beyond your team, it contains a private\n",
        "key that is tied to your Roboflow account. Replace the XXXX with the appropriate link"
      ],
      "metadata": {
        "id": "Ftdctpl0XLKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "b5i0fO1qLcDw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('roboflow.zip'):\n",
        "    !curl -L \"https://universe.roboflow.com/ds/XXXX\" > roboflow.zip;\n",
        "    !unzip roboflow.zip;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml-Z6jUwU6Vw"
      },
      "source": [
        "Create a folder inference if it doesnt exist. In this folder we copy all image files collected from drone of coconut farm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "3GI5E8ONVB54"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('inference'):\n",
        "    !mkdir inference\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp /content/drive/My\\ Drive/inference/*.jpg /content/inference/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KOm7tnmLkyz"
      },
      "source": [
        "The dataset is organized into three folders: train, valid (validation), and test. Each of these folders contains two subfolders: images and labels. Metadata about the dataset is stored in data.yaml.\n",
        "\n",
        "The dataset is structured in the following manner:\n",
        "\n",
        "1.   data.yaml\n",
        "2.   test\n",
        "*   images\n",
        "*   labels\n",
        "3. train\n",
        "*   images\n",
        "*   labels\n",
        "4. valid\n",
        "*   images\n",
        "*   labels\n",
        "\n",
        "The labels directory contains label.txt files that consists of annotation data. YOLOv5 Pytorch normalised format is as follows:\n",
        "\n",
        "\"class_label center_x center_y width, height\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax"
      ],
      "metadata": {
        "id": "9UBvbVCDgViU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization** is the process of scaling data to a specific range, typically between 0 and 1. This is done to ensure that all features contribute equally to the model's learning process, preventing features with larger values from dominating those with smaller values. It also helps improve the stability and speed of training.\n",
        "\n",
        "**In the context of YOLOv5's bounding box annotations:**\n",
        "\n",
        "**center_x and center_y:** The center coordinates of the bounding box are normalized by dividing the pixel coordinates by the image width and height, respectively.\n",
        "**width and height:** The width and height of the bounding box are normalized by dividing the pixel dimensions by the image width and height, respectively.\n",
        "\n",
        "This ensures all values are within the 0-1 range, regardless of the original image size. Let's say you have an image that is 640 pixels wide and 480 pixels high. You want to annotate a dog in the image. The bounding box around the dog has the following pixel coordinates:\n",
        "\n",
        "**Top-left corner:** (100, 150) Bottom-right corner: (400, 350) To calculate the normalized YOLOv5 format:\n",
        "\n",
        "**Center_x:** (100 + 400) / 2 = 250. 250 / 640 (image width) = 0.39 Center_y: (150 + 350) / 2 = 250. 250 / 480 (image height) = 0.52 Width: 400 - 100 = 300. 300 / 640 (image width) = 0.47 Height: 350 - 150 = 200. 200 / 480 (image height) = 0.42 Assuming the class_label for \"dog\" is 0, the annotation line in the label.txt file would be:\n",
        "\n",
        "0 0.39 0.52 0.47 0.42\n",
        "\n",
        "Visualize a Few Ground Truth Images Before moving forward, let's check out few of the ground truth images. The current annotations in the text files are in normalized [x_center, y_center, width, height] format. Let's write a function that will convert it back to [x_min, y_min, x_max, y_max] format.\n",
        "\n",
        "Before proceeding, let's examine some of the correct, labeled images (ground truth). The image labels are currently in a normalized format representing the center point, width, and height of a bounding box. We need to convert these labels to a format representing the top-left and bottom-right corners of the bounding box.\n",
        "\n",
        "The dataset YAML file, named data.yaml, includes important information for training a model. It specifies the locations of the training, validation, and test images, as well as their corresponding labels.\n",
        "\n",
        "**Key components of the file are:**\n",
        "\n",
        "* *train:* Path to the directory containing training images (../train/images).\n",
        "**test:* Path to the directory for test images (../test/images).\n",
        "**val:* Path to the directory for validation images (../valid/images).\n",
        "**nc:* Number of classes in the dataset (1 class).\n",
        "**names:* List of class names, which in this case contains only one class: 'drone'.\n",
        "This structured format helps in organizing data for machine learning tasks."
      ],
      "metadata": {
        "id": "bsBVJoNFdXFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "1gF5nRyBL6No"
      },
      "outputs": [],
      "source": [
        "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
        "def yolo2bbox(bboxes):\n",
        "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
        "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
        "    return xmin, ymin, xmax, ymax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "o9af89UUMIU8"
      },
      "outputs": [],
      "source": [
        "class_names = ['dry', 'green', 'tender']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization"
      ],
      "metadata": {
        "id": "ugyg1mkchC9N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbhL01hNMZMS"
      },
      "source": [
        "##Render bounding box around the objects according to the annotated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o-rIUKxAMaU1"
      },
      "outputs": [],
      "source": [
        "def plot_box(image, bboxes, labels):\n",
        "    # Need the image height and width to denormalize the bounding box coordinates.\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    for box_num, box in enumerate(bboxes):\n",
        "        # Obtain top_left and bottom_right corners.\n",
        "        x1, y1, x2, y2 = yolo2bbox(box)\n",
        "        # denormalize the coordinates.\n",
        "        xmin = int(x1*w)\n",
        "        ymin = int(y1*h)\n",
        "        xmax = int(x2*w)\n",
        "        ymax = int(y2*h)\n",
        "        # Width and height of the bounding box.\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        class_name = class_names[int(labels[box_num])]\n",
        "\n",
        "        # Render bounding box around the object.\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color=(0,0,255), thickness=max(2, int(w/275)))\n",
        "\n",
        "        # Render labels.\n",
        "        font_scale = min(1, max(4,int(w/500)))\n",
        "        font_thickness = min(1, max(10,int(w/50)))\n",
        "        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
        "\n",
        "        # Text width and height.\n",
        "        tw, th = cv2.getTextSize(class_name, 0, fontScale=font_scale, thickness=font_thickness)[0]\n",
        "\n",
        "        p2 = p1[0] + tw, p1[1] + -th - 10\n",
        "\n",
        "        cv2.rectangle(image, p1, p2, color=(255,0,0), thickness=-1)\n",
        "        cv2.putText(image, class_name, (xmin+1, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuwhYGy-J5sc"
      },
      "source": [
        "##Visualize random images with the bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_oRI1duMxOL"
      },
      "outputs": [],
      "source": [
        "def visualize(image_paths, label_paths, num_samples):\n",
        "    all_training_images = glob.glob(image_paths)\n",
        "    all_training_labels = glob.glob(label_paths)\n",
        "    all_training_images.sort()\n",
        "    all_training_labels.sort()\n",
        "\n",
        "    num_images = len(all_training_images)\n",
        "\n",
        "    plt.figure(figsize=(20, 17))\n",
        "    for i in range(num_samples):\n",
        "        j = random.randint(0, num_images-1)\n",
        "        image = cv2.imread(all_training_images[j])\n",
        "        with open(all_training_labels[j], 'r') as f:\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            label_lines = f.readlines()\n",
        "            for label_line in label_lines:\n",
        "                label = label_line[0]\n",
        "                bbox_string = label_line[2:]\n",
        "                bbox_string\n",
        "                x_c, y_c, w, h = bbox_string.split(' ' )\n",
        "                x_c = float(x_c)\n",
        "                y_c = float(y_c)\n",
        "                w = float(w)\n",
        "                h = float(h)\n",
        "                bboxes.append([x_c, y_c, w, h])\n",
        "                labels.append(label)\n",
        "        result_image = plot_box(image, bboxes, labels)\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.imshow(result_image[:, :, ::-1])\n",
        "        plt.axis('off')\n",
        "    plt.subplots_adjust(wspace=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# Visualize a few training images.\n",
        "visualize(\n",
        "    image_paths='train/images/*',\n",
        "    label_paths='train/labels/*',\n",
        "    num_samples=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "-oXUdoaxhw9y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmQUVy_QM-zv"
      },
      "source": [
        "##Clone YOLOV5 Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GGKQxbzlNPF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b86a13-dc54-40be-8939-b8abaa0fafec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17022, done.\u001b[K\n",
            "remote: Total 17022 (delta 0), reused 0 (delta 0), pack-reused 17022 (from 1)\u001b[K\n",
            "Receiving objects: 100% (17022/17022), 15.62 MiB | 32.58 MiB/s, done.\n",
            "Resolving deltas: 100% (11694/11694), done.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('yolov5'):\n",
        "    !git clone https://github.com/ultralytics/yolov5.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "S9kxvwq0l4OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e247f-c928-45a6-c594-bfc7edc9bc9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5/yolov5/yolov5\n",
            "/content/yolov5/yolov5/yolov5\n"
          ]
        }
      ],
      "source": [
        "%cd yolov5/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "hdfn61i_mFtw",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc0950e-2f5f-43d0-82e7-5c9a9c4bdb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.43)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.6)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (8.3.27)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model"
      ],
      "metadata": {
        "id": "KN5nPq1ai1Lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting your training, sign up for a Weights &amp; Biases (wandb) account at wandb.ai using your\n",
        "GitHub account. If you already have a wandb account, select the &quot;use existing account&quot; option,\n",
        "authenticate, and obtain your API key. You&#39;ll be prompted to enter this API key during the training\n",
        "setup process."
      ],
      "metadata": {
        "id": "bv9KXnjPuS-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "iRixr-ODmTTE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3591a17-24f8-4830-b258-affc52456557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/__init__.py\", line 25, in <module>\n",
            "    from .artifacts.artifact import Artifact\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/artifacts/artifact.py\", line 37, in <module>\n",
            "    from wandb.apis.normalize import normalize_exceptions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/apis/__init__.py\", line 44, in <module>\n",
            "    from .public import Api as PublicApi  # noqa\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/apis/public/__init__.py\", line 1, in <module>\n",
            "    from wandb.apis.public.api import Api, RetryingClient, requests\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/apis/public/api.py\", line 32, in <module>\n",
            "    from wandb.sdk.launch.utils import LAUNCH_DEFAULT_PROJECT\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/launch/__init__.py\", line 1, in <module>\n",
            "    from ._launch import launch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/launch/_launch.py\", line 12, in <module>\n",
            "    from . import loader\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/launch/loader.py\", line 7, in <module>\n",
            "    from wandb.docker import is_docker_installed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/docker/__init__.py\", line 10, in <module>\n",
            "    from wandb.docker import auth, www_authenticate\n",
            "  File \"<frozen importlib._bootstrap>\", line 1024, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 170, in __enter__\n",
            "  File \"<frozen importlib._bootstrap>\", line 196, in _get_module_lock\n",
            "  File \"<frozen importlib._bootstrap>\", line 71, in __init__\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/yolov5/train.py\", line 81, in <module>\n",
            "    from utils.loggers import LOGGERS, Loggers\n",
            "  File \"/content/yolov5/yolov5/utils/loggers/__init__.py\", line 14, in <module>\n",
            "    from utils.loggers.wandb.wandb_utils import WandbLogger\n",
            "  File \"/content/yolov5/yolov5/utils/loggers/wandb/wandb_utils.py\", line 25, in <module>\n",
            "    import wandb\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/__init__.py\", line 21, in <module>\n",
            "    from wandb import sdk as wandb_sdk\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data ../data.yaml --weights yolov5m.pt --img 640 --epochs {50} --batch-size 32\n",
        "#!python train.py --data ../data.yaml --weights yolov5m.pt --img 640 --epochs 25 --batch-size 32 --device 0 --nosave!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga44hxUneyk"
      },
      "source": [
        "#Testing & Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-8GMe7tngNe"
      },
      "outputs": [],
      "source": [
        "!python val.py --task 'test' --weights runs/train/exp/weights/best.pt --data ../data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu19NqJ14Q4c"
      },
      "source": [
        "In this section, we will carry out inference on unseen images and videos from the internet. Note that the inferece_results folder will change with every run. Make sure to change the path if you are running the block more than once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV_-Ueau4SRU"
      },
      "outputs": [],
      "source": [
        "# Run inference.\n",
        "!python detect.py --source ../inference --weights runs/train/exp/weights/best.pt --project inference_results --name .\n",
        "plt.figure(figsize=(25,15))\n",
        "for i in range(3):\n",
        "  img = cv2.imread('./inference_results/coconut-test{}.jpg'.format(i+1))\n",
        "  plt.subplot(1, 3, i+1);\n",
        "  plt.imshow(img[...,::-1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXoI4A_s4xav"
      },
      "source": [
        "##Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpQbJ13x4yUf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('runs/train/exp2/weights/best.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-oXUdoaxhw9y",
        "MmQUVy_QM-zv",
        "KN5nPq1ai1Lm",
        "jga44hxUneyk"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}